{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spark-submit --master yarn --deploy-mode cluster  --conf spark.yarn.submit.waitAppCompletion=false --conf spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.speculation=false --conf spark.executorEnv.LANG=en_US.UTF-8 --conf spark.yarn.appMasterEnv.LANG=en_US.UTF-8 --driver-cores 20 --driver-memory 20G --num-executors 40 --executor-cores 15 --executor-memory 25G ./covid/py/get-accuracy-statistics-pyspark.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3920.072"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1585595574642-1585591654570)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf,desc,row_number,col,year,month,dayofmonth,dayofweek,to_timestamp,size,isnan,lit,date_format,to_timestamp\n",
    "from pyspark.sql.types import MapType, StringType, IntegerType, StructType, StructField, FloatType, ArrayType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    spark=SparkSession.builder.appName(\"\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "source='cuebiq'\n",
    "country='US'\n",
    "\n",
    "if os.getenv('CLUSTER')=='PRINCE':\n",
    "    path_to_data='/scratch/spf248/covid/data'\n",
    "    directory='2020010100'\n",
    "    file='part-00000-0428e20d-9019-4cbf-b5ce-bc9414007fec-c000.csv.gz'\n",
    "else:\n",
    "    path_to_data='/user/spf248/covid/data'\n",
    "    directory='*'\n",
    "    file='*'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Dataset\n"
     ]
    }
   ],
   "source": [
    "print('Load Dataset')\n",
    "\n",
    "schema= StructType([\n",
    "StructField(\"_c0\", FloatType(), False),\n",
    "StructField(\"_c1\", StringType(), False),\n",
    "StructField(\"_c2\", FloatType(), False),\n",
    "StructField(\"_c3\", FloatType(), False),\n",
    "StructField(\"_c4\", FloatType(), False),\n",
    "StructField(\"_c5\", FloatType(), False),\n",
    "StructField(\"_c6\", FloatType(), False),\n",
    "StructField(\"_c7\", StringType(), False),\n",
    "StructField(\"_c8\", StringType(), False),])\n",
    "\n",
    "df=spark.read.option(\n",
    "'compression', 'gzip').option(\n",
    "'header', 'false').option(\n",
    "\"multiLine\", \"true\").option(\n",
    "'escape','\"').option(\n",
    "\"encoding\", \"UTF-8\").option(\n",
    "\"delimiter\", \"\\t\").schema(schema).csv(\n",
    "os.path.join(\n",
    "path_to_data,\n",
    "source,\n",
    "country,\n",
    "directory,\n",
    "file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess Dataset\n"
     ]
    }
   ],
   "source": [
    "print('Preprocess Dataset')\n",
    "\n",
    "column_names=[\n",
    "'timestamp',\n",
    "'cuebiq_id',\n",
    "'device_type',\n",
    "'latitude',\n",
    "'longitude',\n",
    "'accuracy',\n",
    "'time_zone_offset',\n",
    "'classification_type',\n",
    "'transformation_type']\n",
    "df=df.toDF(*column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Statistics\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy Statistics')\n",
    "accuracy=df.groupBy('accuracy').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Save')\n",
    "accuracy.write.mode(\"overwrite\").parquet(os.path.join(path_to_data,source,country,'accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|accuracy|count|\n",
      "+--------+-----+\n",
      "|  5039.0|    1|\n",
      "| 11189.0|    1|\n",
      "|  2976.0|    2|\n",
      "|  1575.0|    8|\n",
      "|   714.0|   11|\n",
      "|  2679.0|    3|\n",
      "|  2098.0|    4|\n",
      "|  2318.0|   11|\n",
      "|  1033.0|    2|\n",
      "|  3238.0|    1|\n",
      "|  3599.0|    2|\n",
      "| 11378.0|    1|\n",
      "|  5750.0|    1|\n",
      "|  7460.0|    1|\n",
      "|  3675.0|    1|\n",
      "| 11827.0|    1|\n",
      "|  1393.0|    5|\n",
      "|  1585.0|    4|\n",
      "|  1081.0|    5|\n",
      "|   550.0|    2|\n",
      "+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# accuracy.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
