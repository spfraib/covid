{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spark-submit --master yarn --deploy-mode cluster  --conf spark.yarn.appMasterEnv.SPARK_HOME=/share/apps/spark/^Cark-2.4.0-bin-hadoop2.6 --conf spark.yarn.submit.waitAppCompletion=false --conf spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.speculation=false --conf spark.executorEnv.LANG=en_US.UTF-8 --conf spark.yarn.appMasterEnv.LANG=en_US.UTF-8 --driver-cores 20 --driver-memory 40G --num-executors 40 --executor-cores 15 --executor-memory 40G ./covid/py/get-summary-statistics.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf,desc,row_number,col,year,month,hour,dayofmonth,dayofweek,to_timestamp,size,isnan,lit,date_format,to_timestamp,struct\n",
    "from pyspark.sql.types import MapType, StringType, IntegerType, StructType, StructField, FloatType, ArrayType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    spark=SparkSession.builder.appName(\"\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source='cuebiq'\n",
    "country='MX'\n",
    "n_chunks=1\n",
    "start_date='2020-01-01'\n",
    "end_date=datetime.today().strftime('%Y-%m-%d')\n",
    "if os.getenv('CLUSTER')=='PRINCE':\n",
    "    path_to_data='/scratch/spf248/covid/data'\n",
    "    directories=directories[:1]\n",
    "else:\n",
    "    path_to_data='/user/spf248/covid/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List data files\n",
    "paths=[]\n",
    "fs=spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
    "directories=[x.strftime('%Y-%m-%d').replace('-','')+'00' for x in pd.date_range(start_date,end_date)]\n",
    "for directory in directories:\n",
    "    path_to_directory=os.path.join(path_to_data,source,'s3',country,directory)\n",
    "    if not fs.exists(spark._jvm.org.apache.hadoop.fs.Path(path_to_directory)):\n",
    "        continue\n",
    "    list_status=fs.listStatus(spark._jvm.org.apache.hadoop.fs.Path(path_to_directory))\n",
    "    paths.extend([file.getPath().toString().replace('hdfs://dumbo','').replace('file:','') for file in list_status])\n",
    "    paths=sorted([path for path in paths if '.csv.gz' in path])\n",
    "    print(directory)\n",
    "    \n",
    "# Split files list into chunks to avoid memory issues\n",
    "if os.getenv('CLUSTER')=='PRINCE':\n",
    "    paths=paths[:1]\n",
    "    paths_chunks=np.array_split(paths,n_chunks)\n",
    "    paths_chunks=paths_chunks[:1]\n",
    "else:\n",
    "    paths_chunks=np.array_split(paths,n_chunks)\n",
    "print('# Files:', sum([len(paths_chunk) for paths_chunk in paths_chunks]))\n",
    "print('# Chunks:', len(paths_chunks))\n",
    "    \n",
    "schema= StructType([\n",
    "StructField(\"_c0\", FloatType(), False),\n",
    "StructField(\"_c1\", StringType(), False),\n",
    "StructField(\"_c2\", FloatType(), False),\n",
    "StructField(\"_c3\", FloatType(), False),\n",
    "StructField(\"_c4\", FloatType(), False),\n",
    "StructField(\"_c5\", FloatType(), False),\n",
    "StructField(\"_c6\", FloatType(), False),\n",
    "StructField(\"_c7\", StringType(), False),\n",
    "StructField(\"_c8\", StringType(), False),])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data By Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(paths_chunk):\n",
    "\n",
    "    df=spark.read.option(\n",
    "    'compression', 'gzip').option(\n",
    "    'header', 'false').option(\n",
    "    \"multiLine\", \"true\").option(\n",
    "    'escape','\"').option(\n",
    "    \"encoding\", \"UTF-8\").option(\n",
    "    \"delimiter\", \"\\t\").schema(schema).csv(list(paths_chunk))\n",
    "\n",
    "    column_names=[\n",
    "    'timestamp',\n",
    "    'cuebiq_id',\n",
    "    'device_type',\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    'accuracy',\n",
    "    'time_zone_offset',\n",
    "    'classification_type',\n",
    "    'transformation_type']\n",
    "    df=df.toDF(*column_names)\n",
    "\n",
    "    df=df.withColumn(\n",
    "    \"time\",to_timestamp(df[\"timestamp\"]+df[\"time_zone_offset\"])).withColumn(\n",
    "    \"date\", date_format(col(\"time\"), \"yyyy-MM-dd\")).withColumn(\n",
    "    'hour',hour(\"time\")).withColumn(\n",
    "    'point', struct('longitude','latitude'))\n",
    "    \n",
    "    return df.select('cuebiq_id','device_type','time','date','hour','point','classification_type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Statistics by Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,paths_chunk in enumerate(paths_chunks):\n",
    "    \n",
    "    df=load_data(paths_chunk)\n",
    "    df.cache()\n",
    "    \n",
    "    if not i:\n",
    "        \n",
    "        device_id=df.groupby('cuebiq_id').agg(\n",
    "        {'device_type':'first'}).withColumnRenamed('first(device_type)','device_type')\n",
    "        \n",
    "        n_pings_id_date_hour=df.groupby(\n",
    "        'cuebiq_id','date','hour').count().withColumnRenamed('count','n_pings')\n",
    "        \n",
    "        n_pings_id_personal_date_hour=df.filter(df['classification_type']=='PERSONAL_AREA').groupby(\n",
    "        'cuebiq_id','point','date','hour').count().withColumnRenamed('count','n_pings')\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        device_id=device_id.unionByName(\n",
    "        df.groupby('cuebiq_id').agg({'device_type':'first'}).withColumnRenamed('first(device_type)','device_type'))\n",
    "        \n",
    "        n_pings_id_date_hour=n_pings_id_date_hour.unionByName(\n",
    "        df.groupby('cuebiq_id','date','hour').count().withColumnRenamed('count','n_pings'))\n",
    "        \n",
    "        n_pings_id_personal_date_hour=n_pings_id_personal_date_hour.unionByName(\n",
    "        df.filter(df['classification_type']=='PERSONAL_AREA').groupby(\n",
    "        'cuebiq_id','point','date','hour').count().withColumnRenamed('count','n_pings'))\n",
    "        \n",
    "    df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_id=device_id.groupby('cuebiq_id').agg({'device_type':'first'}).withColumnRenamed('first(device_type)','device_type')\n",
    "n_pings_id_date_hour=n_pings_id_date_hour.groupby('cuebiq_id','date','hour').agg({'n_pings':'sum'}).withColumnRenamed('sum(n_pings)','n_pings')\n",
    "n_pings_id_personal_date_hour=n_pings_id_personal_date_hour.groupby('cuebiq_id','point','date','hour').agg({'n_pings':'sum'}).withColumnRenamed('sum(n_pings)','n_pings')\n",
    "n_pings_id_day_hour=n_pings_id_date_hour.withColumn('dayofweek',date_format(\"date\",\"u\")).groupby('cuebiq_id','dayofweek','hour').agg({'n_pings':'sum'}).withColumnRenamed('sum(n_pings)','n_pings')\n",
    "n_pings_id_personal_day_hour=n_pings_id_personal_date_hour.withColumn('dayofweek',date_format(\"date\",\"u\")).groupby('cuebiq_id','point','dayofweek','hour').agg({'n_pings':'sum'}).withColumnRenamed('sum(n_pings)','n_pings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_id.write.mode(\"overwrite\").parquet(os.path.join(path_to_data,source,'processed',country,'device_id'))\n",
    "n_pings_id_date_hour.write.mode(\"overwrite\").parquet(os.path.join(path_to_data,source,'processed',country,'n_pings_id_date_hour'))\n",
    "n_pings_id_day_hour.write.mode(\"overwrite\").parquet(os.path.join(path_to_data,source,'processed',country,'n_pings_id_day_hour'))\n",
    "n_pings_id_personal_date_hour.write.mode(\"overwrite\").parquet(os.path.join(path_to_data,source,'processed',country,'n_pings_id_personal_date_hour'))\n",
    "n_pings_id_personal_day_hour.write.mode(\"overwrite\").parquet(os.path.join(path_to_data,source,'processed',country,'n_pings_id_personal_day_hour'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
