{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spark-submit --master yarn --deploy-mode cluster  --conf spark.yarn.appMasterEnv.SPARK_HOME=/share/apps/spark/^Cark-2.4.0-bin-hadoop2.6 --conf spark.yarn.submit.waitAppCompletion=false --conf spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.speculation=false --conf spark.executorEnv.LANG=en_US.UTF-8 --conf spark.yarn.appMasterEnv.LANG=en_US.UTF-8 --driver-cores 20 --driver-memory 40G --num-executors 40 --executor-cores 15 --executor-memory 40G ./covid/py/get-summary-statistics.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf,desc,row_number,col,year,month,hour,dayofmonth,dayofweek,to_timestamp,size,isnan,lit,date_format,to_timestamp,struct\n",
    "from pyspark.sql.types import MapType, StringType, IntegerType, StructType, StructField, FloatType, ArrayType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    spark=SparkSession.builder.appName(\"\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020010100\n",
      "# Files: 1\n",
      "# Chunks: 1\n"
     ]
    }
   ],
   "source": [
    "source='cuebiq'\n",
    "country='ID'\n",
    "n_chunks=1\n",
    "start_date='2020-01-01'\n",
    "end_date=datetime.today().strftime('%Y-%m-%d')\n",
    "directories=[x.strftime('%Y-%m-%d').replace('-','')+'00' for x in pd.date_range(start_date,end_date)]\n",
    "fs=spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
    "\n",
    "if os.getenv('CLUSTER')=='PRINCE':\n",
    "    path_to_data='/scratch/spf248/covid/data'\n",
    "    directories=directories[:1]\n",
    "else:\n",
    "    path_to_data='/user/spf248/covid/data'\n",
    "    \n",
    "paths=[]\n",
    "for directory in directories:\n",
    "    path_to_directory=os.path.join(path_to_data,source,'s3',country,directory)\n",
    "    if not fs.exists(spark._jvm.org.apache.hadoop.fs.Path(path_to_directory)):\n",
    "        continue\n",
    "    list_status=fs.listStatus(spark._jvm.org.apache.hadoop.fs.Path(path_to_directory))\n",
    "    paths.extend([file.getPath().toString().replace('hdfs://dumbo','').replace('file:','') for file in list_status])\n",
    "    paths=sorted([path for path in paths if '.csv.gz' in path])\n",
    "    print(directory)\n",
    "    \n",
    "if os.getenv('CLUSTER')=='PRINCE':\n",
    "    paths=paths[:1]\n",
    "    paths_chunks=np.array_split(paths,n_chunks)\n",
    "    paths_chunks=paths_chunks[:1]\n",
    "else:\n",
    "    paths_chunks=np.array_split(paths,n_chunks)\n",
    "\n",
    "print('# Files:', sum([len(paths_chunk) for paths_chunk in paths_chunks]))\n",
    "print('# Chunks:', len(paths_chunks))\n",
    "    \n",
    "schema= StructType([\n",
    "StructField(\"_c0\", FloatType(), False),\n",
    "StructField(\"_c1\", StringType(), False),\n",
    "StructField(\"_c2\", FloatType(), False),\n",
    "StructField(\"_c3\", FloatType(), False),\n",
    "StructField(\"_c4\", FloatType(), False),\n",
    "StructField(\"_c5\", FloatType(), False),\n",
    "StructField(\"_c6\", FloatType(), False),\n",
    "StructField(\"_c7\", StringType(), False),\n",
    "StructField(\"_c8\", StringType(), False),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(paths_chunk):\n",
    "\n",
    "    df=spark.read.option(\n",
    "    'compression', 'gzip').option(\n",
    "    'header', 'false').option(\n",
    "    \"multiLine\", \"true\").option(\n",
    "    'escape','\"').option(\n",
    "    \"encoding\", \"UTF-8\").option(\n",
    "    \"delimiter\", \"\\t\").schema(schema).csv(list(paths_chunk))\n",
    "\n",
    "    column_names=[\n",
    "    'timestamp',\n",
    "    'cuebiq_id',\n",
    "    'device_type',\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    'accuracy',\n",
    "    'time_zone_offset',\n",
    "    'classification_type',\n",
    "    'transformation_type']\n",
    "    df=df.toDF(*column_names)\n",
    "\n",
    "    df=df.withColumn(\n",
    "    \"time\",to_timestamp(df[\"timestamp\"]+df[\"time_zone_offset\"])).withColumn(\n",
    "    \"date\", date_format(col(\"time\"), \"yyyy-MM-dd\")).withColumn(\n",
    "    'dayofweek',date_format(\"time\",\"u\")).withColumn('hour',hour(\"time\")).withColumn(\n",
    "    'point', struct('longitude','latitude'))\n",
    "    \n",
    "    return df.select('cuebiq_id','device_type','time','date','dayofweek','hour','point','classification_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,paths_chunk in enumerate(paths_chunks):\n",
    "    \n",
    "    df=load_data(paths_chunk)\n",
    "    df.cache()\n",
    "    \n",
    "    if not i:\n",
    "        \n",
    "        n_pings_id=df.groupby('cuebiq_id').agg(\n",
    "        {'device_type':'first'}).withColumnRenamed('first(device_type)','device_type')\n",
    "        \n",
    "        n_pings_id_date=df.groupby('cuebiq_id','date').count().withColumnRenamed('count','n_pings')\n",
    "        \n",
    "        n_pings_id_personal=df.filter(df['classification_type']=='PERSONAL_AREA').groupby(\n",
    "        'cuebiq_id','point').count().withColumnRenamed('count','n_pings')\n",
    "            \n",
    "        n_pings_id_personal_date=df.filter(df['classification_type']=='PERSONAL_AREA').groupby(\n",
    "        'cuebiq_id','point','date').count().withColumnRenamed('count','n_pings')\n",
    "        \n",
    "        n_pings_id_personal_day_hour=df.filter(df['classification_type']=='PERSONAL_AREA').groupby(\n",
    "        'cuebiq_id','point','dayofweek','hour').count().withColumnRenamed('count','n_pings')\n",
    "        \n",
    "        n_pings_id_day_hour=df.groupby(\n",
    "        'cuebiq_id','dayofweek','hour').count().withColumnRenamed('count','n_pings')\n",
    "    else:\n",
    "        \n",
    "        n_pings_id=n_pings_id.unionByName(\n",
    "        df.groupby('cuebiq_id').agg({'device_type':'first'}).withColumnRenamed('first(device_type)','device_type'))\n",
    "        \n",
    "        n_pings_id_date=n_pings_id_date.unionByName(\n",
    "        df.groupby('cuebiq_id','date').count().withColumnRenamed('count','n_pings'))\n",
    "        \n",
    "        n_pings_id_personal=n_pings_id_personal.unionByName(\n",
    "        df.filter(df['classification_type']=='PERSONAL_AREA').groupby(\n",
    "        'cuebiq_id','point').count().withColumnRenamed('count','n_pings'))\n",
    "        \n",
    "        n_pings_id_personal_date=n_pings_id_personal_date.unionByName(\n",
    "        df.filter(df['classification_type']=='PERSONAL_AREA').groupby(\n",
    "        'cuebiq_id','point','date').count().withColumnRenamed('count','n_pings'))\n",
    "        \n",
    "        n_pings_id_personal_day_hour=n_pings_id_personal_day_hour.unionByName(\n",
    "        df.filter(df['classification_type']=='PERSONAL_AREA').groupby(\n",
    "        'cuebiq_id','point','dayofweek','hour').count().withColumnRenamed('count','n_pings'))\n",
    "        \n",
    "        n_pings_id_day_hour=n_pings_id_day_hour.unionByName(df.groupby(\n",
    "        'cuebiq_id','dayofweek','hour').count().withColumnRenamed('count','n_pings'))\n",
    "        \n",
    "    df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pings_id=n_pings_id.groupby('cuebiq_id').agg({'device_type':'first'}).withColumnRenamed('first(device_type)','device_type')\n",
    "n_pings_id_date=n_pings_id_date.groupby('cuebiq_id','date').agg({'n_pings':'sum'}).withColumnRenamed('sum(n_pings)','n_pings')\n",
    "n_pings_id_personal=n_pings_id_personal.groupby('cuebiq_id','point').agg({'n_pings':'sum'}).withColumnRenamed('sum(n_pings)','n_pings')\n",
    "n_pings_id_personal_date=n_pings_id_personal_date.groupby('cuebiq_id','point','date').agg({'n_pings':'sum'}).withColumnRenamed('sum(n_pings)','n_pings')\n",
    "n_pings_id_personal_day_hour=n_pings_id_personal_day_hour.groupby('cuebiq_id','point','dayofweek','hour').agg({'n_pings':'sum'}).withColumnRenamed('sum(n_pings)','n_pings')\n",
    "n_pings_id_day_hour=n_pings_id_day_hour.groupby('cuebiq_id','dayofweek','hour').agg({'n_pings':'sum'}).withColumnRenamed('sum(n_pings)','n_pings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pings_id_date.write.mode(\"overwrite\").parquet(os.path.join(path_to_data,source,'aggregates',country,'n_pings_id_date'))\n",
    "n_pings_id_personal.write.mode(\"overwrite\").parquet(os.path.join(path_to_data,source,'aggregates',country,'n_pings_id_personal'))\n",
    "n_pings_id_personal_date.write.mode(\"overwrite\").parquet(os.path.join(path_to_data,source,'aggregates',country,'n_pings_id_personal_date'))\n",
    "n_pings_id_personal_day_hour.write.mode(\"overwrite\").parquet(os.path.join(path_to_data,source,'aggregates',country,'n_pings_id_personal_day_hour'))\n",
    "n_pings_id_day_hour.write.mode(\"overwrite\").parquet(os.path.join(path_to_data,source,'aggregates',country,'n_pings_id_day_hour'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "users=(n_pings_id_date.groupby('cuebiq_id').agg(\n",
    "{'date':'count','n_pings':'sum'}).withColumnRenamed(\n",
    "'count(date)','n_days').withColumnRenamed('sum(n_pings)','n_pings')).join(\n",
    "n_pings_id_personal.groupby('cuebiq_id').count().withColumnRenamed(\n",
    "'count','n_personal'),on=['cuebiq_id']).join(n_pings_id,on=['cuebiq_id'])\n",
    "users.write.mode(\"overwrite\").parquet(os.path.join(path_to_data,source,'aggregates',country,'users_profile'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-------+----------+-----------+\n",
      "|           cuebiq_id|n_days|n_pings|n_personal|device_type|\n",
      "+--------------------+------+-------+----------+-----------+\n",
      "|5476c2d50fa3396a6...|     3|     17|         1|        0.0|\n",
      "|a28a817158e80e35d...|     2|     36|         1|        0.0|\n",
      "|39d3c113a30f1fd7a...|     2|    179|         1|        1.0|\n",
      "|ba5844efcdf7b415a...|     2|    135|         1|        1.0|\n",
      "|2b3f9d1040ff00bef...|     2|    381|         1|        1.0|\n",
      "|80212d453ce93ce85...|     2|    662|         2|        1.0|\n",
      "|b2e54fa9d2420ed40...|     2|     80|         2|        1.0|\n",
      "|c05b4cf1328f36cce...|     2|    234|         2|        0.0|\n",
      "|a63778ecb57bbe24c...|     2|     56|         1|        1.0|\n",
      "|8eb9cdc18ed67ce85...|     2|     29|         2|        0.0|\n",
      "|21f619f1cc118d432...|     2|    185|         2|        1.0|\n",
      "|20132c25e188c1543...|     2|    159|         1|        1.0|\n",
      "|3415decfb70d9cb42...|     2|     37|         1|        0.0|\n",
      "|0e59242866089fc17...|     2|     39|         1|        0.0|\n",
      "|557871263c4d9be8c...|     2|    122|         1|        1.0|\n",
      "|942edcc8e4e6154cb...|     2|    102|         1|        1.0|\n",
      "|319ac7e19dbb8c3eb...|     2|     52|         1|        0.0|\n",
      "|4783b007d24a259e4...|     2|     51|         1|        1.0|\n",
      "|dbb4df9ddd9eff8a8...|     1|     25|         1|        1.0|\n",
      "|e2d52b21af9ee21be...|     2|     27|         1|        0.0|\n",
      "+--------------------+------+-------+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
