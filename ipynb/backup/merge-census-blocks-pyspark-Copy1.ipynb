{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://datasystemslab.github.io/GeoSpark/tutorial/geospark-sql-python/\n",
    "\n",
    "https://github.com/DataSystemsLab/GeoSpark/tree/master/python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spark-submit --master yarn --deploy-mode cluster  --conf spark.yarn.appMasterEnv.SPARK_HOME=/share/apps/spark/spark-2.4.0-bin-hadoop2.6 --conf spark.yarn.submit.waitAppCompletion=false --conf spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.speculation=false --conf spark.executorEnv.LANG=en_US.UTF-8 --conf spark.yarn.appMasterEnv.LANG=en_US.UTF-8 --driver-cores 20 --driver-memory 55G --num-executors 30 --executor-cores 15 --executor-memory 30G ./covid/py/merge-admin-blocks-pyspark.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf,desc,row_number,col,year,month,dayofmonth,dayofweek,to_timestamp,size,isnan,lit,date_format,from_json\n",
    "from pyspark.sql.types import MapType, StringType, IntegerType, StructType, StructField, FloatType, ArrayType, DoubleType\n",
    "\n",
    "from geospark.register import GeoSparkRegistrator\n",
    "from geospark.utils import GeoSparkKryoRegistrator, KryoSerializer\n",
    "from geospark.register import upload_jars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upload_jars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Spark\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    print('Create Spark')\n",
    "    spark=SparkSession.builder.appName(\"\").config(\n",
    "    \"spark.serializer\", KryoSerializer.getName).config(\n",
    "    \"spark.kryo.registrator\", GeoSparkKryoRegistrator.getName).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GeoSparkRegistrator.registerAll(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "source='cuebiq'\n",
    "country='US'\n",
    "start_date='2020-01-01'\n",
    "end_date='2020-03-19'\n",
    "accuracy_threshold=100\n",
    "\n",
    "states=['AK', 'AL', 'AR', 'AS', 'AZ', 'CA', 'CO', 'CT', 'DC', 'DE', 'FL', 'GA', 'GU', 'HI', 'IA', 'ID', 'IL', 'IN', 'KS', 'KY', 'LA', 'MA', 'MD', 'ME', 'MI', 'MN', 'MO', 'MS', 'MT', 'NC', 'ND', 'NE', 'NH', 'NJ', 'NM', 'NV', 'NY', 'OH', 'OK', 'OR', 'PA', 'PR', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VA', 'VI', 'VT', 'WA', 'WI', 'WV', 'WY']\n",
    "\n",
    "if os.getenv('CLUSTER')=='PRINCE':\n",
    "    path_to_data='/scratch/spf248/covid/data'\n",
    "    directories=['*'+str(x)+'00' for x in range(10)][1:2]\n",
    "    file='part-00000-0428e20d-9019-4cbf-b5ce-bc9414007fec-c000.csv.gz'\n",
    "    states=['NJ']\n",
    "else:\n",
    "    path_to_data='/user/spf248/covid/data'\n",
    "    directories=['*'+str(x)+'00' for x in range(10)]\n",
    "    file='*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema= StructType([\n",
    "StructField(\"_c0\", DoubleType(), False),\n",
    "StructField(\"_c1\", StringType(), False),\n",
    "StructField(\"_c2\", IntegerType(), False),\n",
    "StructField(\"_c3\", FloatType(), False),\n",
    "StructField(\"_c4\", FloatType(), False),\n",
    "StructField(\"_c5\", DoubleType(), False),\n",
    "StructField(\"_c6\", DoubleType(), False),\n",
    "StructField(\"_c7\", StringType(), False),\n",
    "StructField(\"_c8\", StringType(), False),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pings(directory):\n",
    "    \n",
    "    pings=spark.read.option(\n",
    "    'compression', 'gzip').option(\n",
    "    'header','false').option(\n",
    "    \"multiLine\", \"true\").option(\n",
    "    'escape','\"').option(\n",
    "    \"encoding\", \"UTF-8\").option(\n",
    "    \"delimiter\", \"\\t\").schema(schema).csv(\n",
    "    os.path.join(\n",
    "    path_to_data,\n",
    "    source,\n",
    "    country,\n",
    "    directory,\n",
    "    file))\n",
    "\n",
    "    column_names=[\n",
    "    'timestamp',\n",
    "    'cuebiq_id',\n",
    "    'device_type',\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    'accuracy',\n",
    "    'time_zone_offset',\n",
    "    'classification_type',\n",
    "    'transformation_type']\n",
    "    pings=pings.toDF(*column_names)\n",
    "    pings=pings.withColumn(\"time\",to_timestamp(pings[\"timestamp\"]+pings[\"time_zone_offset\"]))\n",
    "    pings=pings.withColumn(\"date\", date_format(col(\"time\"),\"yyyy-MM-dd\"))\n",
    "    pings=pings.filter(pings[\"date\"]>=lit(start_date)).filter(pings[\"date\"]<=lit(end_date))\n",
    "    pings=pings.filter(pings[\"accuracy\"]<=lit(accuracy_threshold))\n",
    "    pings.createOrReplaceTempView(\"pings\")\n",
    "\n",
    "    pings=spark.sql(\"\"\"select time\n",
    "    , cuebiq_id\n",
    "    , latitude\n",
    "    , longitude\n",
    "    , accuracy\n",
    "    , classification_type\n",
    "    , ST_Point(cast(pings.longitude as Decimal(24,20))\n",
    "    , cast(pings.latitude as Decimal(24,20))) as point\n",
    "    from pings\n",
    "    \"\"\")\n",
    "    pings.createOrReplaceTempView(\"pings\")\n",
    "    \n",
    "    return pings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_admin(state):\n",
    "    \n",
    "    admin=spark.read.option(\n",
    "    \"header\", \"true\").csv(\n",
    "    os.path.join(\n",
    "    path_to_data,\n",
    "    'shapefiles',\n",
    "    country,\n",
    "    'polygons',\n",
    "    state+'.csv'))\n",
    "    admin.createOrReplaceTempView(\"admin\")\n",
    "\n",
    "    admin=spark.sql(\"\"\"select admin.GEOID10 as GEOID10\n",
    "    , ST_GeomFromText(admin.geometry) as polygon\n",
    "    from admin\n",
    "    \"\"\")\n",
    "    admin.createOrReplaceTempView(\"admin\")\n",
    "    \n",
    "    return admin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for directory in directories:\n",
    "    \n",
    "    pings=load_pings(directory)\n",
    "    \n",
    "    for state in states:\n",
    "        \n",
    "        admin=load_admin(state)\n",
    "\n",
    "        spatial_join=spark.sql(\n",
    "        \"\"\"\n",
    "            SELECT p.time\n",
    "            , p.cuebiq_id\n",
    "            , p.latitude\n",
    "            , p.longitude\n",
    "            , p.accuracy\n",
    "            , p.classification_type\n",
    "            , s.GEOID10\n",
    "            FROM pings AS p, admin AS s\n",
    "            WHERE ST_Intersects(p.point, s.polygon)\n",
    "        \"\"\"\n",
    "        )\n",
    "        spatial_join.write.mode(\"overwrite\").parquet(os.path.join(path_to_data,source,country,state,directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------+---------+--------+-------------------+---------------+\n",
      "|               time|           cuebiq_id|latitude|longitude|accuracy|classification_type|        GEOID10|\n",
      "+-------------------+--------------------+--------+---------+--------+-------------------+---------------+\n",
      "|2020-01-01 11:02:25|222a36f677448d751...|40.86519|-74.13686|    15.0|      PERSONAL_AREA|340311755003010|\n",
      "|2020-01-01 11:22:13|222a36f677448d751...|40.86519|-74.13686|     8.0|      PERSONAL_AREA|340311755003010|\n",
      "|2020-01-01 11:22:12|222a36f677448d751...|40.86519|-74.13686|     8.0|      PERSONAL_AREA|340311755003010|\n",
      "|2020-01-01 07:24:52|222a36f677448d751...|40.86519|-74.13686|     5.0|      PERSONAL_AREA|340311755003010|\n",
      "|2020-01-01 07:22:56|222a36f677448d751...|40.86519|-74.13686|    15.0|      PERSONAL_AREA|340311755003010|\n",
      "|2020-01-01 10:32:25|222a36f677448d751...|40.86519|-74.13686|    15.0|      PERSONAL_AREA|340311755003010|\n",
      "|2020-01-01 08:11:22|222a36f677448d751...|40.86519|-74.13686|    15.0|      PERSONAL_AREA|340311755003010|\n",
      "|2020-01-01 07:25:38|222a36f677448d751...|40.86519|-74.13686|    15.0|      PERSONAL_AREA|340311755003010|\n",
      "|2020-01-01 08:42:44|222a36f677448d751...|40.86519|-74.13686|    15.0|      PERSONAL_AREA|340311755003010|\n",
      "|2020-01-01 07:48:40|222a36f677448d751...|40.86519|-74.13686|     8.0|      PERSONAL_AREA|340311755003010|\n",
      "|2020-01-01 09:18:04|222a36f677448d751...|40.86519|-74.13686|    14.0|      PERSONAL_AREA|340311755003010|\n",
      "|2020-01-01 11:11:41|222a36f677448d751...|40.86519|-74.13686|    13.0|      PERSONAL_AREA|340311755003010|\n",
      "|2020-01-01 09:33:25|222a36f677448d751...|40.86519|-74.13686|    14.0|      PERSONAL_AREA|340311755003010|\n",
      "|2020-01-01 08:52:43|222a36f677448d751...|40.86519|-74.13686|     7.0|      PERSONAL_AREA|340311755003010|\n",
      "|2020-01-01 07:24:46|222a36f677448d751...|40.86519|-74.13686|    15.0|      PERSONAL_AREA|340311755003010|\n",
      "|2020-01-01 07:25:05|222a36f677448d751...|40.86519|-74.13686|    15.0|      PERSONAL_AREA|340311755003010|\n",
      "|2020-01-01 11:41:58|222a36f677448d751...|40.86519|-74.13686|    15.0|      PERSONAL_AREA|340311755003010|\n",
      "|2020-01-01 11:51:42|222a36f677448d751...|40.86519|-74.13686|    15.0|      PERSONAL_AREA|340311755003010|\n",
      "|2020-01-01 08:31:00|222a36f677448d751...|40.86519|-74.13686|    15.0|      PERSONAL_AREA|340311755003010|\n",
      "|2020-01-01 09:50:08|222a36f677448d751...|40.86519|-74.13686|     7.0|      PERSONAL_AREA|340311755003010|\n",
      "+-------------------+--------------------+--------+---------+--------+-------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spatial_join.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
