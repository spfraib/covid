{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf,desc,row_number,col,year,month,hour,dayofmonth,dayofweek,to_timestamp,size,isnan,lit,date_format,to_timestamp,struct\n",
    "from pyspark.sql.types import MapType, StringType, IntegerType, StructType, StructField, FloatType, ArrayType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "90*(1586053727192-1586052435239)/(1000*3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    spark=SparkSession.builder.appName(\"\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020010100\n",
      "# Files: 2000\n",
      "2020010200\n",
      "# Files: 4000\n",
      "2020010300\n",
      "# Files: 6000\n",
      "2020010400\n",
      "# Files: 8000\n",
      "2020010500\n",
      "# Files: 10000\n",
      "2020010600\n",
      "# Files: 12000\n",
      "2020010700\n",
      "# Files: 14000\n",
      "2020010800\n",
      "# Files: 16000\n",
      "2020010900\n",
      "# Files: 18000\n",
      "2020011000\n",
      "# Files: 20000\n",
      "2020011100\n",
      "# Files: 22000\n",
      "2020011200\n",
      "# Files: 24000\n",
      "2020011300\n",
      "# Files: 26000\n",
      "2020011400\n",
      "# Files: 28000\n",
      "2020011500\n",
      "# Files: 30000\n",
      "2020011600\n",
      "# Files: 32000\n",
      "2020011700\n",
      "# Files: 34000\n",
      "2020011800\n",
      "# Files: 36000\n",
      "2020011900\n",
      "# Files: 38000\n",
      "2020012000\n",
      "# Files: 40000\n",
      "2020012100\n",
      "# Files: 42000\n",
      "2020012200\n",
      "# Files: 44000\n",
      "2020012300\n",
      "# Files: 46000\n",
      "2020012400\n",
      "# Files: 48000\n",
      "2020012500\n",
      "# Files: 50000\n",
      "2020012600\n",
      "# Files: 52000\n",
      "2020012700\n",
      "# Files: 54000\n",
      "2020012800\n",
      "# Files: 56000\n",
      "2020012900\n",
      "# Files: 58000\n",
      "2020013000\n",
      "# Files: 60000\n",
      "2020013100\n",
      "# Files: 62000\n",
      "2020020100\n",
      "# Files: 64000\n",
      "2020020200\n",
      "# Files: 66000\n",
      "2020020300\n",
      "# Files: 68000\n",
      "2020020400\n",
      "# Files: 70000\n",
      "2020020500\n",
      "# Files: 72000\n",
      "2020020600\n",
      "# Files: 74000\n",
      "2020020700\n",
      "# Files: 76000\n",
      "2020020800\n",
      "# Files: 78000\n",
      "2020020900\n",
      "# Files: 80000\n",
      "2020021000\n",
      "# Files: 82000\n",
      "2020021100\n",
      "# Files: 84000\n",
      "2020021200\n",
      "# Files: 86000\n",
      "2020021300\n",
      "# Files: 88000\n",
      "2020021400\n",
      "# Files: 90000\n",
      "2020021500\n",
      "# Files: 92000\n",
      "2020021600\n",
      "# Files: 94000\n",
      "2020021700\n",
      "# Files: 96000\n",
      "2020021800\n",
      "# Files: 98000\n",
      "2020021900\n",
      "# Files: 100000\n",
      "2020022000\n",
      "# Files: 102000\n",
      "2020022100\n",
      "# Files: 104000\n",
      "2020022200\n",
      "# Files: 106000\n",
      "2020022300\n",
      "# Files: 108000\n",
      "2020022400\n",
      "# Files: 110000\n",
      "2020022500\n",
      "# Files: 112000\n",
      "2020022600\n",
      "# Files: 114000\n",
      "2020022700\n",
      "# Files: 116000\n",
      "2020022800\n",
      "# Files: 118000\n",
      "2020022900\n",
      "# Files: 120000\n",
      "2020030100\n",
      "# Files: 122000\n",
      "2020030200\n",
      "# Files: 124000\n",
      "2020030300\n",
      "# Files: 126000\n",
      "2020030400\n",
      "# Files: 128000\n",
      "2020030500\n",
      "# Files: 130000\n",
      "2020030600\n",
      "# Files: 132000\n",
      "2020030700\n",
      "# Files: 134000\n",
      "2020030800\n",
      "# Files: 136000\n",
      "2020030900\n",
      "# Files: 138000\n",
      "2020031000\n",
      "# Files: 140000\n",
      "2020031100\n",
      "# Files: 142000\n",
      "2020031200\n",
      "# Files: 144000\n",
      "2020031300\n",
      "# Files: 146000\n",
      "2020031400\n",
      "# Files: 148000\n",
      "2020031500\n",
      "# Files: 150000\n",
      "2020031600\n",
      "# Files: 152000\n",
      "2020031700\n",
      "# Files: 154000\n",
      "2020031800\n",
      "# Files: 156000\n",
      "2020031900\n",
      "# Files: 158000\n",
      "2020032000\n",
      "# Files: 160000\n",
      "2020032100\n",
      "# Files: 162000\n",
      "2020032200\n",
      "# Files: 164000\n",
      "2020032300\n",
      "# Files: 166000\n",
      "2020032400\n",
      "# Files: 168000\n",
      "2020032500\n",
      "# Files: 170000\n",
      "2020032600\n",
      "# Files: 172000\n",
      "2020032700\n",
      "# Files: 174000\n",
      "2020032800\n",
      "# Files: 176000\n",
      "2020032900\n",
      "# Files: 178000\n",
      "# Chunks: 30\n"
     ]
    }
   ],
   "source": [
    "source='cuebiq'\n",
    "country='MX'\n",
    "n_chunks=1#30\n",
    "cutoff_night=22\n",
    "cutoff_morning=8\n",
    "start_date='2020-01-01'\n",
    "end_date=datetime.today().strftime('%Y-%m-%d')\n",
    "directories=[x.strftime('%Y-%m-%d').replace('-','')+'00' for x in pd.date_range(start_date,end_date)]\n",
    "fs=spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
    "\n",
    "if os.getenv('CLUSTER')=='PRINCE':\n",
    "    path_to_data='/scratch/spf248/covid/data'\n",
    "    directories=directories[:1]\n",
    "else:\n",
    "    path_to_data='/user/spf248/covid/data'\n",
    "#     directories=directories[:1]\n",
    "    \n",
    "paths=[]\n",
    "for directory in directories:\n",
    "    \n",
    "    path_to_directory=os.path.join(\n",
    "    path_to_data,\n",
    "    source,\n",
    "    's3',\n",
    "    country,\n",
    "    directory)\n",
    "    \n",
    "    if not fs.exists(spark.sparkContext._jvm.org.apache.hadoop.fs.Path(path_to_directory)):\n",
    "        continue\n",
    "        \n",
    "    list_status=fs.listStatus(spark._jvm.org.apache.hadoop.fs.Path(path_to_directory))\n",
    "    print(directory)\n",
    "\n",
    "    paths.extend([file.getPath().toString().replace('hdfs://dumbo','').replace('file:','') for file in list_status])\n",
    "    paths=sorted([path for path in paths if '.csv.gz' in path])\n",
    "    print('# Files:', len(paths))\n",
    "\n",
    "if os.getenv('CLUSTER')=='PRINCE':\n",
    "    paths=paths[:n_chunks]\n",
    "\n",
    "print('# Chunks:', n_chunks)\n",
    "paths_chunks=np.array_split(paths, n_chunks)\n",
    "\n",
    "schema= StructType([\n",
    "StructField(\"_c0\", FloatType(), False),\n",
    "StructField(\"_c1\", StringType(), False),\n",
    "StructField(\"_c2\", FloatType(), False),\n",
    "StructField(\"_c3\", FloatType(), False),\n",
    "StructField(\"_c4\", FloatType(), False),\n",
    "StructField(\"_c5\", FloatType(), False),\n",
    "StructField(\"_c6\", FloatType(), False),\n",
    "StructField(\"_c7\", StringType(), False),\n",
    "StructField(\"_c8\", StringType(), False),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(paths_chunk):\n",
    "\n",
    "    df=spark.read.option(\n",
    "    'compression', 'gzip').option(\n",
    "    'header', 'false').option(\n",
    "    \"multiLine\", \"true\").option(\n",
    "    'escape','\"').option(\n",
    "    \"encoding\", \"UTF-8\").option(\n",
    "    \"delimiter\", \"\\t\").schema(schema).csv(list(paths_chunk))\n",
    "    \n",
    "    column_names=[\n",
    "    'timestamp',\n",
    "    'cuebiq_id',\n",
    "    'device_type',\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    'accuracy',\n",
    "    'time_zone_offset',\n",
    "    'classification_type',\n",
    "    'transformation_type']\n",
    "    df=df.toDF(*column_names)\n",
    "\n",
    "    df=df.withColumn(\"time\",to_timestamp(df[\"timestamp\"]+df[\"time_zone_offset\"]))\n",
    "    df=df.withColumn(\"date\", date_format(col(\"time\"), \"yyyy-MM-dd\"))\n",
    "    df=df.withColumn('dayofweek',date_format(\"time\",\"u\"))\n",
    "    df=df.withColumn('hour',hour(\"time\")) \n",
    "    df=df.withColumn('point', struct('longitude','latitude'))\n",
    "    \n",
    "    return df.select('cuebiq_id','device_type','time','date','dayofweek','hour','point','classification_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,paths_chunk in enumerate(paths_chunks):\n",
    "    \n",
    "    df=load_data(paths_chunk)\n",
    "    df.cache()\n",
    "    \n",
    "    if not i:\n",
    "        \n",
    "        device_type=df.select('cuebiq_id','device_type').drop_duplicates(subset=['cuebiq_id'])\n",
    "        \n",
    "        daily_pings=df.groupby('cuebiq_id','date').count().withColumnRenamed('count','n_pings')\n",
    "        \n",
    "        daily_personal=df.filter(df['classification_type']=='PERSONAL_AREA').groupby(\n",
    "        'cuebiq_id','point','date').count().withColumnRenamed('count','n_pings')\n",
    "        \n",
    "        hourly_personal=df.filter(df['classification_type']=='PERSONAL_AREA').filter(df['dayofweek']<=5).groupby(\n",
    "        'cuebiq_id','point','hour').count().withColumnRenamed('count','n_pings')\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        device_type=device_type.unionByName(\n",
    "        df.select('cuebiq_id','device_type')).drop_duplicates(subset=['cuebiq_id'])\n",
    "        \n",
    "        daily_pings=daily_pings.unionByName(\n",
    "        df.groupby('cuebiq_id','date').count().withColumnRenamed('count','n_pings'))\n",
    "        \n",
    "        daily_personal=daily_personal.unionByName(df.filter(\n",
    "        df['classification_type']=='PERSONAL_AREA').groupby(\n",
    "        'cuebiq_id','point','date').count().withColumnRenamed('count','n_pings'))\n",
    "        \n",
    "        hourly_personal=hourly_personal.unionByName(\n",
    "        df.filter(df['classification_type']=='PERSONAL_AREA').filter(df['dayofweek']<=5).groupby(\n",
    "        'cuebiq_id','point','hour').count().withColumnRenamed('count','n_pings'))\n",
    "        \n",
    "    df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[cuebiq_id: string, point: struct<longitude:float,latitude:float>, hour: int, n_pings: bigint]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_pings=daily_pings.groupby('cuebiq_id','date').agg({'n_pings':'sum'}).withColumnRenamed('sum(n_pings)','n_pings')\n",
    "daily_pings.write.mode(\"overwrite\").parquet(os.path.join(path_to_data,source,'aggregates',country,'daily_pings'))\n",
    "\n",
    "daily_personal=daily_personal.groupby('cuebiq_id','point','date').agg({'n_pings':'sum'}).withColumnRenamed('sum(n_pings)','n_pings')\n",
    "daily_personal.write.mode(\"overwrite\").parquet(os.path.join(path_to_data,source,'aggregates',country,'daily_personal'))\n",
    "\n",
    "hourly_personal=hourly_personal.groupby('cuebiq_id','point','hour').agg({'n_pings':'sum'}).withColumnRenamed('sum(n_pings)','n_pings')\n",
    "hourly_personal.write.mode(\"overwrite\").parquet(os.path.join(path_to_data,source,'aggregates',country,'hourly_personal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_personal.cache()\n",
    "\n",
    "users=daily_pings.groupby('cuebiq_id').agg(\n",
    "{'date':'count','n_pings':'sum'}).withColumnRenamed(\n",
    "'count(date)','n_days').withColumnRenamed('sum(n_pings)','n_pings')\n",
    "\n",
    "# print('# Users:', users.count())\n",
    "\n",
    "users=users.join(daily_personal.drop_duplicates(\n",
    "subset=['cuebiq_id','point']).groupby('cuebiq_id').agg(\n",
    "{'point':'count'}).withColumnRenamed('count(point)','n_personal'),on='cuebiq_id')\n",
    "\n",
    "# print('# Users:', users.count())\n",
    "\n",
    "users=users.join(device_type,on='cuebiq_id')\n",
    "\n",
    "# print('# Users:', users.count())\n",
    "\n",
    "users.write.mode(\"overwrite\").parquet(os.path.join(path_to_data,source,'aggregates',country,'users'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_personal.unpersist()\n",
    "hourly_personal.cache()\n",
    "\n",
    "users_personal=daily_personal.groupby('cuebiq_id','point').agg(\n",
    "{'n_pings':'sum'}).withColumnRenamed('sum(n_pings)','n_pings')\n",
    "\n",
    "# print('# Personal Locations:', users_personal.count())\n",
    "\n",
    "users_personal=users_personal.join(hourly_personal.filter((\n",
    "hourly_personal['hour']>=cutoff_night)|(hourly_personal['hour']<=cutoff_morning)).groupby(\n",
    "'cuebiq_id','point').agg({'n_pings':'sum'}).withColumnRenamed('sum(n_pings)','n_weeknights'),\n",
    "on=['cuebiq_id','point'])\n",
    "\n",
    "# print('# Personal Locations:', users_personal.count())\n",
    "\n",
    "hourly_personal_max=hourly_personal.groupby('cuebiq_id','point').agg(\n",
    "{'n_pings':'max'}).withColumnRenamed('max(n_pings)','n_pings')\n",
    "\n",
    "users_personal=users_personal.join(\n",
    "hourly_personal.join(hourly_personal_max,on=['cuebiq_id','point','n_pings']).drop_duplicates(\n",
    "subset=['cuebiq_id','point','n_pings']).drop('n_pings').withColumnRenamed(\n",
    "'hour','most_freq_hour'),on=['cuebiq_id','point'])\n",
    "\n",
    "# print('# Personal Locations:', users_personal.count())\n",
    "\n",
    "users_personal.write.mode(\"overwrite\").parquet(os.path.join(path_to_data,source,'aggregates',country,'users_personal'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
