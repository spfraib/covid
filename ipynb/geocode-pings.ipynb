{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://datasystemslab.github.io/GeoSpark/tutorial/geospark-sql-python/\n",
    "\n",
    "https://github.com/DataSystemsLab/GeoSpark/tree/master/python\n",
    "\n",
    "https://datasystemslab.github.io/GeoSpark/tutorial/geospark-core-python/\n",
    "\n",
    "https://medium.com/@karijdempsey/efficient-geospatial-analysis-with-spark-363ba50c5248\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spark-submit --master yarn --deploy-mode cluster  --conf spark.yarn.appMasterEnv.SPARK_HOME=/share/apps/spark/spark-2.4.0-bin-hadoop2.6 --conf spark.yarn.submit.waitAppCompletion=false --conf spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.speculation=false --conf spark.executorEnv.LANG=en_US.UTF-8 --conf spark.yarn.appMasterEnv.LANG=en_US.UTF-8 --driver-cores 20 --driver-memory 55G --num-executors 40 --executor-cores 15 --executor-memory 55G ./covid/py/merge-census-blocks-pyspark.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf,desc,row_number,col,year,month,dayofmonth,dayofweek,to_timestamp,size,isnan,lit,date_format,from_json,broadcast\n",
    "from pyspark.sql.types import MapType, StringType, IntegerType, StructType, StructField, FloatType, ArrayType, DoubleType\n",
    "\n",
    "from geospark.register import GeoSparkRegistrator\n",
    "from geospark.utils import GeoSparkKryoRegistrator, KryoSerializer\n",
    "from geospark.register import upload_jars\n",
    "from geospark.utils.adapter import Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upload_jars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Spark\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    print('Create Spark')\n",
    "    spark=SparkSession.builder.appName(\"\").config(\n",
    "    \"spark.serializer\", KryoSerializer.getName).config(\n",
    "    \"spark.kryo.registrator\", GeoSparkKryoRegistrator.getName).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GeoSparkRegistrator.registerAll(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020010100\n",
      "# Files: 1\n",
      "# Chunks: 1\n"
     ]
    }
   ],
   "source": [
    "source='cuebiq'\n",
    "country='MX'\n",
    "admin_id='ageb'\n",
    "n_chunks=10\n",
    "start_date='2020-01-01'\n",
    "end_date=datetime.today().strftime('%Y-%m-%d')\n",
    "directories=[x.strftime('%Y-%m-%d').replace('-','')+'00' for x in pd.date_range(start_date,end_date)]\n",
    "fs=spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
    "\n",
    "if os.getenv('CLUSTER')=='PRINCE':\n",
    "    path_to_data='/scratch/spf248/covid/data'\n",
    "    directories=directories[:1]\n",
    "else:\n",
    "    path_to_data='/user/spf248/covid/data'\n",
    "    \n",
    "paths=[]\n",
    "for directory in directories:\n",
    "    path_to_directory=os.path.join(path_to_data,source,'s3',country,directory)\n",
    "    if not fs.exists(spark._jvm.org.apache.hadoop.fs.Path(path_to_directory)):\n",
    "        continue\n",
    "    list_status=fs.listStatus(spark._jvm.org.apache.hadoop.fs.Path(path_to_directory))\n",
    "    paths.extend([file.getPath().toString().replace('hdfs://dumbo','').replace('file:','') for file in list_status])\n",
    "    paths=sorted([path for path in paths if '.csv.gz' in path])\n",
    "    print(directory)\n",
    "    \n",
    "if os.getenv('CLUSTER')=='PRINCE':\n",
    "    paths=paths[:1]\n",
    "    paths_chunks=np.array_split(paths,n_chunks)\n",
    "    paths_chunks=paths_chunks[:1]\n",
    "else:\n",
    "    paths_chunks=np.array_split(paths,n_chunks)\n",
    "\n",
    "print('# Files:', sum([len(paths_chunk) for paths_chunk in paths_chunks]))\n",
    "print('# Chunks:', len(paths_chunks))\n",
    "    \n",
    "schema= StructType([\n",
    "StructField(\"_c0\", FloatType(), False),\n",
    "StructField(\"_c1\", StringType(), False),\n",
    "StructField(\"_c2\", FloatType(), False),\n",
    "StructField(\"_c3\", FloatType(), False),\n",
    "StructField(\"_c4\", FloatType(), False),\n",
    "StructField(\"_c5\", FloatType(), False),\n",
    "StructField(\"_c6\", FloatType(), False),\n",
    "StructField(\"_c7\", StringType(), False),\n",
    "StructField(\"_c8\", StringType(), False),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admin=spark.read.option(\n",
    "\"header\", \"true\").csv(\n",
    "os.path.join(\n",
    "path_to_data,\n",
    "'admin',\n",
    "country,\n",
    "'admin.csv'))\n",
    "admin.createOrReplaceTempView(\"admin\")\n",
    "\n",
    "query=\"select admin.\"+admin_id+\" as \"+admin_id+\", ST_GeomFromText(admin.geometry) as polygon from admin\"\n",
    "admin=spark.sql(query)\n",
    "admin.createOrReplaceTempView(\"admin\")\n",
    "admin.cache()\n",
    "print('# Admin Units:',admin.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geocode_pings(paths_chunk):\n",
    "\n",
    "    pings=spark.read.option(\n",
    "    'compression', 'gzip').option(\n",
    "    'header', 'false').option(\n",
    "    \"multiLine\", \"true\").option(\n",
    "    'escape','\"').option(\n",
    "    \"encoding\", \"UTF-8\").option(\n",
    "    \"delimiter\", \"\\t\").schema(schema).csv(list(paths_chunk))\n",
    "\n",
    "    column_names=[\n",
    "    'timestamp',\n",
    "    'cuebiq_id',\n",
    "    'device_type',\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    'accuracy',\n",
    "    'time_zone_offset',\n",
    "    'classification_type',\n",
    "    'transformation_type']\n",
    "    pings=pings.toDF(*column_names)\n",
    "\n",
    "    pings=pings.withColumn(\"time\",to_timestamp(pings[\"timestamp\"]+pings[\"time_zone_offset\"]))\n",
    "    pings.createOrReplaceTempView(\"pings\")\n",
    "    pings=spark.sql(\"\"\"select time\n",
    "    , cuebiq_id\n",
    "    , latitude\n",
    "    , longitude\n",
    "    , accuracy\n",
    "    , classification_type\n",
    "    , ST_Point(cast(pings.longitude as Decimal(24,20))\n",
    "    , cast(pings.latitude as Decimal(24,20))) as point\n",
    "    from pings\n",
    "    \"\"\")\n",
    "    \n",
    "    pings.createOrReplaceTempView(\"pings\")\n",
    "    query=\"\"\"SELECT p.time\n",
    "    , p.cuebiq_id\n",
    "    , p.latitude\n",
    "    , p.longitude\n",
    "    , p.accuracy\n",
    "    , p.classification_type\n",
    "    , s.\"\"\"+admin_id+\"\"\" \n",
    "    FROM pings AS p, admin AS s WHERE ST_Intersects(p.point, s.polygon)\"\"\"\n",
    "    pings_geocoded=spark.sql(query)\n",
    "    \n",
    "    return pings_geocoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(paths_chunks)):\n",
    "    \n",
    "    print('Chunk:', i)\n",
    "    \n",
    "    pings_geocoded=geocode_pings(paths_chunks[i])\n",
    "\n",
    "    pings_geocoded.write.mode(\"overwrite\").parquet(\n",
    "    os.path.join(path_to_data,source,'pings_geocoded',country,str(i)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
